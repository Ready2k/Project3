#!/usr/bin/env python3
"""
Test script for agent responsibility generation.

This tests that agent responsibilities are now generated by LLM to describe
what the agent will DO rather than just repeating the user's requirement.
"""

import asyncio
import sys
from pathlib import Path
from unittest.mock import AsyncMock

# Add the app directory to Python path
sys.path.insert(0, str(Path(__file__).parent))

from app.services.agentic_recommendation_service import AgenticRecommendationService


class MockLLMProvider:
    """Mock LLM provider for testing agent responsibility generation."""
    
    def __init__(self, mock_responses):
        self.mock_responses = mock_responses
        self.call_count = 0
    
    async def generate(self, prompt, purpose=None):
        """Mock generate method."""
        if purpose == "agent_responsibility_generation":
            if self.call_count < len(self.mock_responses):
                response = self.mock_responses[self.call_count]
                self.call_count += 1
                return response
            else:
                return "Autonomous agent responsible for automating the described workflow"
        else:
            # For other purposes (like agent name generation), return a simple response
            return "Order Management Agent"


async def test_agent_responsibility_generation():
    """Test that agent responsibilities are properly generated."""
    
    # Mock LLM responses for different scenarios
    mock_responses = [
        # Tyre order processing
        "Autonomous agent responsible for automating tyre order processing, vehicle compatibility verification, inventory checking, and work order generation for workshop mechanics",
        
        # Invoice processing
        "Autonomous agent responsible for extracting invoice data, validating payment terms, routing approvals based on business rules, and updating accounting systems automatically",
        
        # Customer support
        "Autonomous agent responsible for analyzing support tickets, categorizing issues by priority and type, providing automated responses, and escalating complex cases to human agents"
    ]
    
    # Create mock provider and service
    mock_provider = MockLLMProvider(mock_responses)
    service = AgenticRecommendationService(
        llm_provider=mock_provider,
        pattern_library_path=None  # Not needed for this test
    )
    
    # Test cases
    test_cases = [
        {
            "name": "Tyre Order Processing",
            "requirements": {
                "description": "I need a solution that can take my tyre order and pass it to the team in the workshop, today a customer walks in and we get their car reg and enter it into a system, this tell us what tyres will fit the car. Then we check the stock and if we carry the tyres then we tell the mechanic what to fit."
            },
            "agent_name": "Tyre Order Management Agent"
        },
        {
            "name": "Invoice Processing", 
            "requirements": {
                "description": "I need help processing invoices that come in via email, extracting the data, and getting approvals"
            },
            "agent_name": "Invoice Processing Agent"
        },
        {
            "name": "Customer Support",
            "requirements": {
                "description": "I want to automate our customer support ticket handling and routing"
            },
            "agent_name": "Support Ticket Agent"
        }
    ]
    
    print("🧪 Testing Agent Responsibility Generation")
    print("=" * 60)
    
    for i, test_case in enumerate(test_cases, 1):
        print(f"\n{i}. {test_case['name']}")
        print(f"   User Requirement: {test_case['requirements']['description'][:100]}...")
        
        try:
            # Generate agent responsibility
            responsibility = await service._generate_agent_responsibility(
                test_case['requirements'],
                test_case['agent_name']
            )
            
            print(f"   ✅ Generated Responsibility:")
            print(f"      {responsibility}")
            
            # Verify it's not just repeating the user requirement
            user_desc = test_case['requirements']['description'].lower()
            responsibility_lower = responsibility.lower()
            
            # Check if it's describing actions (should contain action words)
            action_words = ['automat', 'process', 'generat', 'validat', 'extract', 'rout', 'updat', 'analyz', 'categor']
            has_actions = any(word in responsibility_lower for word in action_words)
            
            # Check if it's not just repeating the user's words verbatim
            user_words = set(user_desc.split())
            responsibility_words = set(responsibility_lower.split())
            overlap_ratio = len(user_words.intersection(responsibility_words)) / len(user_words)
            
            if has_actions:
                print(f"   ✅ Contains action words (describes what agent will DO)")
            else:
                print(f"   ❌ Missing action words (may not describe agent actions)")
            
            if overlap_ratio < 0.7:  # Less than 70% word overlap with user requirement
                print(f"   ✅ Not just repeating user requirement (overlap: {overlap_ratio:.1%})")
            else:
                print(f"   ⚠️  High overlap with user requirement (overlap: {overlap_ratio:.1%})")
            
        except Exception as e:
            print(f"   ❌ Error: {str(e)}")
            import traceback
            traceback.print_exc()
    
    print("\n" + "=" * 60)
    print("✅ Agent responsibility generation test completed!")
    
    # Test the improvement
    print("\n🔄 BEFORE vs AFTER Comparison")
    print("-" * 40)
    
    tyre_requirement = "I need a solution that can take my tyre order and pass it to the team in the workshop, today a customer walks in and we get their car reg and enter it into a system, this tell us what tyres will fit the car. Then we check the stock and if we carry the tyres then we tell the mechanic what to fit."
    
    print("BEFORE (just repeating requirement):")
    old_responsibility = f"Main autonomous agent responsible for {tyre_requirement}"
    print(f"  {old_responsibility}")
    
    print("\nAFTER (describing agent actions):")
    new_responsibility = mock_responses[0]  # First mock response
    print(f"  {new_responsibility}")
    
    print(f"\n✅ Improvement: Agent responsibility now describes ACTIONS, not just the user's problem!")


if __name__ == "__main__":
    asyncio.run(test_agent_responsibility_generation())